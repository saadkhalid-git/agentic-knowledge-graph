{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Step 0: Environment Setup\n\n## Prerequisites\n\nBefore running this notebook, ensure you have:\n\n1. **Neo4j Database** running locally on port 7687\n2. **OpenAI API Key** for LLM operations\n3. **Python 3.8+** environment\n\nRun the cell below to:\n- Install all Python requirements\n- Set up your environment file (.env)\n- Verify connections to Neo4j and OpenAI",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Environment Setup - Install requirements and verify credentials\n\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\nprint(\"\ud83d\udd27 Environment Setup for PhD Exercise\")\nprint(\"=\"*60)\n\n# Install requirements\nprint(\"\\n\ud83d\udce6 Installing requirements...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"])\nprint(\"\u2705 Requirements installed\")\n\n# Check for .env file (it's in .gitignore so users need to create it)\nenv_path = Path(\".env\")\n\nif not env_path.exists():\n    print(\"\\n\u274c .env file not found (this is expected on first run)\")\n    print(\"\\n\ud83d\udcdd Creating .env from template...\")\n    \n    # Create from template\n    with open(\".env.template\", 'r') as f:\n        template = f.read()\n    \n    with open(\".env\", 'w') as f:\n        f.write(template)\n    \n    print(\"\\n\u26a0\ufe0f ACTION REQUIRED:\")\n    print(\"1. Edit the .env file in the project root\")\n    print(\"2. Add your OpenAI API key\")\n    print(\"3. Add your Neo4j password\")\n    print(\"4. Save the file and re-run this cell\")\n    print(\"\\n\ud83d\udccd File location: .env\")\n    raise SystemExit(\"Please configure .env and re-run this cell\")\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\n# Verify essential credentials\nprint(\"\\n\ud83d\udd0d Verifying credentials...\")\n\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\nneo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n\nif not openai_key or openai_key == \"your-openai-api-key-here\":\n    print(\"\u274c OPENAI_API_KEY not configured in .env\")\n    raise SystemExit(\"Please add your OpenAI API key to .env\")\n\nif not neo4j_password or neo4j_password == \"your-neo4j-password-here\":\n    print(\"\u274c NEO4J_PASSWORD not configured in .env\")\n    raise SystemExit(\"Please add your Neo4j password to .env\")\n\nprint(\"\u2705 Credentials configured\")\n\n# Test connections\nprint(\"\\n\ud83d\udd17 Testing connections...\")\n\n# Test Neo4j\ntry:\n    from src.neo4j_for_adk import graphdb\n    result = graphdb.send_query(\"RETURN 1 as test\")\n    if result['status'] == 'success':\n        print(\"\u2705 Neo4j connected\")\n    else:\n        print(f\"\u274c Neo4j connection failed: {result.get('error')}\")\n        raise SystemExit(\"Check your Neo4j is running and password is correct\")\nexcept Exception as e:\n    print(f\"\u274c Neo4j error: {str(e)[:100]}\")\n    raise SystemExit(\"Ensure Neo4j is running on localhost:7687\")\n\n# Test OpenAI\ntry:\n    from openai import OpenAI\n    client = OpenAI()\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"test\"}],\n        max_tokens=1\n    )\n    print(\"\u2705 OpenAI API connected\")\nexcept Exception as e:\n    print(f\"\u274c OpenAI API error: {str(e)[:100]}\")\n    raise SystemExit(\"Check your OpenAI API key\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2705 SETUP COMPLETE - Ready to run the pipeline!\")\nprint(\"=\"*60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Step 0: Environment Setup and Requirements\n\n## Important: Pre-requisites\n\nTo run this project, you should have already followed the README instructions. You will need:\n\n1. **Neo4j Database** running locally (default: bolt://localhost:7687)\n2. **OpenAI API Key** for LLM operations\n3. **Python 3.8+** environment\n4. **Environment variables** properly configured\n\nLet's verify your environment is set up correctly:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Start with Clean Database\n",
    "\n",
    "## Clear Neo4j Database\n",
    "\n",
    "First, we'll ensure we're starting with a completely empty database to demonstrate the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.neo4j_for_adk import graphdb\n",
    "from notebooks.tools import clear_neo4j_data, drop_neo4j_indexes\n",
    "\n",
    "print(\"\ud83e\uddf9 Clearing Neo4j database...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Drop all indexes first\n",
    "drop_result = drop_neo4j_indexes()\n",
    "print(f\"\ud83d\udccc Indexes dropped: {drop_result['status']}\")\n",
    "\n",
    "# Clear all data\n",
    "clear_result = clear_neo4j_data()\n",
    "print(f\"\ud83d\uddd1\ufe0f Data cleared: {clear_result['status']}\")\n",
    "\n",
    "print(\"\\n\u2705 Database is now empty and ready for pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Database is Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the database is empty\n",
    "check_query = \"MATCH (n) RETURN count(n) as node_count\"\n",
    "result = graphdb.send_query(check_query)\n",
    "\n",
    "if result['status'] == 'success':\n",
    "    count = result['query_result'][0]['node_count']\n",
    "    print(f\"\ud83d\udcca Current database state:\")\n",
    "    print(f\"   Nodes: {count}\")\n",
    "    \n",
    "    rel_check = \"MATCH ()-[r]->() RETURN count(r) as rel_count\"\n",
    "    rel_result = graphdb.send_query(rel_check)\n",
    "    if rel_result['status'] == 'success':\n",
    "        rel_count = rel_result['query_result'][0]['rel_count']\n",
    "        print(f\"   Relationships: {rel_count}\")\n",
    "    \n",
    "    if count == 0 and rel_count == 0:\n",
    "        print(\"\\n\u2705 Confirmed: Database is completely empty\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f Warning: Database still contains data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build the Knowledge Graph\n",
    "\n",
    "## Run the ADK Pipeline\n",
    "\n",
    "Now we'll run the complete pipeline to build our knowledge graph from scratch. This will:\n",
    "1. Analyze CSV and markdown files\n",
    "2. Generate intelligent plans using LLM\n",
    "3. Build domain graph from CSV data\n",
    "4. Extract entities from markdown reviews\n",
    "5. Resolve entities between graphs\n",
    "6. Validate quality at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from src.pipeline.adk_dynamic_builder import ADKDynamicKnowledgeGraphBuilder\n",
    "\n",
    "print(\"\ud83d\ude80 Starting Knowledge Graph Pipeline\")\n",
    "print(\"=\"*60)\n",
    "print(\"This will take 2-3 minutes to complete...\\n\")\n",
    "\n",
    "async def run_pipeline():\n",
    "    \"\"\"Run the complete ADK pipeline.\"\"\"\n",
    "    builder = ADKDynamicKnowledgeGraphBuilder(\n",
    "        data_dir=None,\n",
    "        llm_model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    results = await builder.build_complete_graph(\n",
    "        reset=False,  # We already reset above\n",
    "        force_regenerate_plans=True,\n",
    "        limit_text_files=None,\n",
    "        validate_quality=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the pipeline\n",
    "results = await run_pipeline()\n",
    "\n",
    "# Show results summary\n",
    "if results['status'] == 'success':\n",
    "    print(\"\\n\u2705 Pipeline completed successfully!\")\n",
    "    \n",
    "    # Show statistics\n",
    "    if 'final_statistics' in results:\n",
    "        stats = results['final_statistics']\n",
    "        print(f\"\\n\ud83d\udcca Graph Built:\")\n",
    "        print(f\"   Total Nodes: {stats.get('total_nodes', 0):,}\")\n",
    "        print(f\"   Total Relationships: {stats.get('total_relationships', 0):,}\")\n",
    "    \n",
    "    # Show quality score\n",
    "    if 'quality_metrics' in results:\n",
    "        quality = results['quality_metrics']\n",
    "        print(f\"\\n\ud83c\udfc6 Quality Score: {quality.get('quality_score', 0)}/100\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Pipeline failed: {results.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Graph Construction\n",
    "\n",
    "Let's verify that our knowledge graph now contains both structured (CSV) and unstructured (review) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's now in the graph\n",
    "stats_query = \"\"\"\n",
    "MATCH (n)\n",
    "WITH labels(n)[0] as label, count(n) as count\n",
    "RETURN label, count\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "result = graphdb.send_query(stats_query)\n",
    "if result['status'] == 'success':\n",
    "    print(\"\ud83d\udcca Knowledge Graph Contents:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"{'Entity Type':<15} {'Count':>10}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    total = 0\n",
    "    csv_entities = ['Product', 'Part', 'Supplier', 'Assembly']\n",
    "    review_entities = ['User', 'Rating', 'Issue', 'Feature']\n",
    "    \n",
    "    for row in result['query_result']:\n",
    "        label = row['label']\n",
    "        count = row['count']\n",
    "        \n",
    "        # Mark source\n",
    "        if label in csv_entities:\n",
    "            source = \"(CSV)\"\n",
    "        elif label in review_entities:\n",
    "            source = \"(Reviews)\"\n",
    "        else:\n",
    "            source = \"\"\n",
    "            \n",
    "        print(f\"{label:<15} {count:>10} {source}\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(f\"{'TOTAL':<15} {total:>10}\")\n",
    "    \n",
    "    print(\"\\n\u2705 Graph successfully built with both CSV and review data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Deliverable 1: Architecture Design\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    User Interface                       \u2502\n",
    "\u2502              (Natural Language Queries)                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  Query Engine                           \u2502\n",
    "\u2502         (NL \u2192 Cypher Query Translation)                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                     \u2502\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Neo4j Knowledge Graph                      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502  Domain Graph    \u2502    \u2502   Subject Graph        \u2502   \u2502\n",
    "\u2502  \u2502  (CSV Data)      \u2502\u25c4\u2500\u2500\u2500\u25ba  (Review Entities)     \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502                      \u2502\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Structured Agent     \u2502  \u2502  Unstructured Agent      \u2502\n",
    "\u2502  (CSV Processing)     \u2502  \u2502  (Review Extraction)     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Design Logic\n",
    "\n",
    "1. **Multi-Agent Architecture**: Specialized agents handle different data types\n",
    "   - Structured Agent: Processes CSV files into domain entities\n",
    "   - Unstructured Agent: Extracts entities from markdown reviews using LLM\n",
    "\n",
    "2. **Dual Graph Structure**: Separates concerns while enabling connections\n",
    "   - Domain Graph: Products, Parts, Suppliers, Assemblies (from CSV)\n",
    "   - Subject Graph: Users, Ratings, Issues, Features (from reviews)\n",
    "\n",
    "3. **Entity Resolution**: Links entities across graphs\n",
    "   - Products mentioned in reviews connect to product catalog\n",
    "   - Enables tracing issues back to suppliers\n",
    "\n",
    "4. **Natural Language Interface**: User-friendly query system\n",
    "   - Translates English questions to Cypher graph queries\n",
    "   - Returns answers with evidence and confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 2: Implementation\n",
    "\n",
    "## Initialize Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.query_engine import KnowledgeGraphQueryEngine\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the query engine\n",
    "engine = KnowledgeGraphQueryEngine()\n",
    "print(\"\u2705 Query Engine initialized\")\n",
    "print(\"\u2705 Ready to answer questions about the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3: Demonstration\n",
    "\n",
    "## Answering the Example Questions\n",
    "\n",
    "Now we'll demonstrate our system answering each of the three required questions, showing different capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: What products are available in the catalog?\n",
    "\n",
    "**Capability Demonstrated**: Simple entity listing from structured CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Product Catalog\n",
    "question1 = \"What products are available in the catalog?\"\n",
    "result1 = engine.answer_question(question1)\n",
    "\n",
    "print(f\"\ud83d\udcdd Question: {question1}\")\n",
    "print(f\"\\n\ud83d\udca1 Answer:\\n{result1.answer}\")\n",
    "print(f\"\\n\ud83d\udd0d Evidence: {len(result1.evidence)} products found\")\n",
    "print(f\"\ud83d\udcca Confidence: {result1.confidence:.0%}\")\n",
    "\n",
    "# Show the Cypher query used\n",
    "if result1.query_used:\n",
    "    print(f\"\\n\ud83d\udd27 Query Used (for traceability):\\n{result1.query_used[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: What are customers saying about the Malmo Desk?\n",
    "\n",
    "**Capability Demonstrated**: Entity extraction from unstructured markdown reviews + cross-source linking\n",
    "\n",
    "This question showcases:\n",
    "- **Text Processing**: Extracts entities (users, ratings, issues, features) from markdown\n",
    "- **Entity Resolution**: Links \"Malmo Desk\" from reviews to product catalog\n",
    "- **Aggregation**: Combines multiple reviews into coherent answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Customer Reviews (THE CRITICAL QUESTION)\n",
    "question2 = \"What are customers saying about the Malmo Desk?\"\n",
    "result2 = engine.answer_question(question2)\n",
    "\n",
    "print(f\"\ud83d\udcdd Question: {question2}\")\n",
    "print(f\"\\n\ud83d\udca1 Answer:\\n{result2.answer}\")\n",
    "print(f\"\\n\ud83d\udd0d Evidence: {len(result2.evidence)} data points\")\n",
    "print(f\"\ud83d\udcca Confidence: {result2.confidence:.0%}\")\n",
    "\n",
    "# Demonstrate traceability - show raw evidence\n",
    "if result2.evidence:\n",
    "    print(\"\\n\ud83d\udccb Raw Evidence (Traceability):\")\n",
    "    evidence = result2.evidence[0] if result2.evidence else {}\n",
    "    for key in ['reviewers', 'ratings', 'issues', 'features']:\n",
    "        if key in evidence:\n",
    "            print(f\"  \u2022 {key}: {evidence[key][:3] if isinstance(evidence[key], list) else evidence[key]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Which suppliers provide parts for the Stockholm Chair?\n",
    "\n",
    "**Capability Demonstrated**: Multi-hop graph traversal across structured relationships\n",
    "\n",
    "This question requires:\n",
    "- **4-hop traversal**: Product \u2192 Assembly \u2192 Part \u2192 Supplier\n",
    "- **Data aggregation**: Collecting supplier details\n",
    "- **Relationship following**: Using CONTAINS, IS_PART_OF, SUPPLIES relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Supply Chain Query\n",
    "question3 = \"Which suppliers provide parts for the Stockholm Chair, and what are their contact details?\"\n",
    "result3 = engine.answer_question(question3)\n",
    "\n",
    "print(f\"\ud83d\udcdd Question: {question3}\")\n",
    "print(f\"\\n\ud83d\udca1 Answer:\\n{result3.answer}\")\n",
    "print(f\"\\n\ud83d\udd0d Evidence: {len(result3.evidence)} supplier records\")\n",
    "print(f\"\ud83d\udcca Confidence: {result3.confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Capabilities Demonstration\n",
    "\n",
    "## Multi-Source Pattern Discovery\n",
    "\n",
    "To showcase capabilities not covered by the example questions, let's demonstrate cross-product pattern analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Question: Cross-Product Patterns\n",
    "bonus_question = \"Which products share similar quality issues?\"\n",
    "\n",
    "# Direct Cypher query for pattern discovery\n",
    "pattern_query = \"\"\"\n",
    "MATCH (p1:Product)-[:HAS_ISSUE]->(i:Issue)<-[:HAS_ISSUE]-(p2:Product)\n",
    "WHERE p1.product_name < p2.product_name\n",
    "RETURN p1.product_name as product1,\n",
    "       p2.product_name as product2,\n",
    "       collect(DISTINCT i.description)[0..2] as shared_issues\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "result = graphdb.send_query(pattern_query)\n",
    "if result['status'] == 'success' and result['query_result']:\n",
    "    print(f\"\ud83d\udcdd Bonus Question: {bonus_question}\")\n",
    "    print(\"\\n\ud83d\udca1 Pattern Discovery Results:\")\n",
    "    for row in result['query_result']:\n",
    "        if row.get('shared_issues'):\n",
    "            print(f\"\\n\u2022 {row['product1']} and {row['product2']}\")\n",
    "            print(f\"  Share issues: {', '.join(row['shared_issues'])}\")\n",
    "    \n",
    "    print(\"\\n\ud83c\udfaf Capability: Cross-entity pattern matching across unstructured data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Capabilities Summary\n",
    "\n",
    "## What Our System Can Do\n",
    "\n",
    "1. **Simple Queries** (Q1)\n",
    "   - List entities from structured data\n",
    "   - Direct CSV \u2192 Graph queries\n",
    "\n",
    "2. **Text Analysis** (Q2)\n",
    "   - Extract entities from markdown reviews\n",
    "   - Sentiment and issue identification\n",
    "   - Link reviews to products\n",
    "\n",
    "3. **Multi-Hop Traversal** (Q3)\n",
    "   - 4+ hop graph queries\n",
    "   - Supply chain tracing\n",
    "   - Relationship aggregation\n",
    "\n",
    "4. **Pattern Discovery** (Bonus)\n",
    "   - Cross-product analysis\n",
    "   - Common issue identification\n",
    "   - Quality correlation\n",
    "\n",
    "## Traceability\n",
    "\n",
    "Every answer includes:\n",
    "- **Query Used**: The Cypher query for reproducibility\n",
    "- **Evidence**: Raw data supporting the answer\n",
    "- **Confidence Score**: Reliability metric\n",
    "- **Source Attribution**: Which files/nodes contributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Disclosure\n",
    "\n",
    "## Tools Used\n",
    "\n",
    "1. **Claude 3.5 Sonnet** (Anthropic)\n",
    "   - Architecture design assistance\n",
    "   - Code implementation guidance\n",
    "   - Debugging entity extraction issues\n",
    "   - Documentation writing\n",
    "\n",
    "2. **OpenAI GPT-4** (via API)\n",
    "   - Entity extraction from reviews\n",
    "   - Natural language to Cypher translation\n",
    "   - Query intent classification\n",
    "\n",
    "3. **Google Agent Development Kit (ADK)**\n",
    "   - Multi-agent orchestration framework\n",
    "   - LLM-based validation\n",
    "\n",
    "## How AI Was Applied\n",
    "\n",
    "- **Design Phase**: Used Claude to explore graph vs. vector database approaches\n",
    "- **Implementation**: AI helped debug entity extraction when reviews weren't connecting\n",
    "- **Testing**: AI suggested test cases and multi-hop query examples\n",
    "- **Documentation**: AI assisted in creating clear explanations\n",
    "\n",
    "All core logic and system integration was implemented by the candidate with AI as a development assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Key Achievements\n",
    "\n",
    "\u2705 **Connected disparate data sources** - CSV and markdown unified in single graph\n",
    "\n",
    "\u2705 **Answered all example questions** - With full traceability\n",
    "\n",
    "\u2705 **Demonstrated advanced capabilities** - Pattern discovery, multi-hop traversal\n",
    "\n",
    "\u2705 **Production-ready system** - Scalable, maintainable, extensible\n",
    "\n",
    "## System Statistics\n",
    "\n",
    "- **295 nodes** across 8 entity types\n",
    "- **252 relationships** in 5 types\n",
    "- **77% quality score** from validation\n",
    "- **<1 second** query response time\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "I believe the purpose of the exercise is to assess a candidateâ€™s proficiency with Python and general development skills, I was also able to get solid results by using a notebook with an LLM like ChatGPT. I provided the data files directly and queried them to explore the problem effectively.",
    "Link: https://chatgpt.com/share/69171cf4-0b80-800c-9e02-29b6048800ae",
    "---\n",
    "\n",
    "**Thank you for reviewing this submission!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
